{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Artificial_Vision_Project_Morphing_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "818f74c8bda94a40b42c04adbf5179c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9220cad100b2440596411c777a819415",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0667bd76dac64244966fb9e80158df8f",
              "IPY_MODEL_c16109a7fa0e4f66956c5909fd8fb729",
              "IPY_MODEL_0b3e9ed2c1b14316b2e4806043bb78a6"
            ]
          }
        },
        "9220cad100b2440596411c777a819415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0667bd76dac64244966fb9e80158df8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3f974598d2cb4536bf6af718ac397e96",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_475be7b5453f453dac60cb607314dbcd"
          }
        },
        "c16109a7fa0e4f66956c5909fd8fb729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5f185e712dce4464af37870b77155baa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 244408911,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 244408911,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41de7dcf7fd345e1b1aa35d020c69651"
          }
        },
        "0b3e9ed2c1b14316b2e4806043bb78a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aafdd2f18ab3451db7709445bc73372c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 233M/233M [00:02&lt;00:00, 110MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ab04b14dbba4e19957c05ce54ba0dea"
          }
        },
        "3f974598d2cb4536bf6af718ac397e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "475be7b5453f453dac60cb607314dbcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f185e712dce4464af37870b77155baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41de7dcf7fd345e1b1aa35d020c69651": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aafdd2f18ab3451db7709445bc73372c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ab04b14dbba4e19957c05ce54ba0dea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christvalt/MorphedImageAttactCNN/blob/main/Artificial_Vision_Project_Morphing_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K1qlaan6vqa"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TydsVM2z6vq8"
      },
      "source": [
        "\n",
        "\n",
        "Modello realizzato da zero e Finetuning \n",
        "=============================\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu_D05n2dP9a"
      },
      "source": [
        "Inizializzazione librerie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdMXypLz6vrO",
        "outputId": "c00e7a76-0059-44cf-f1c0-a72a92e09185"
      },
      "source": [
        "from __future__ import print_function \n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Version:  1.9.0+cu102\n",
            "Torchvision Version:  0.10.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sejbaeVBdtQA"
      },
      "source": [
        "# Caricamento dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRfwXucfcY3m",
        "outputId": "bb34577e-9a66-48d8-8d41-931c6c96c940"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEanVJd16vrb"
      },
      "source": [
        "Inputs\n",
        "------\n",
        "\n",
        "In questa parte inizializiamo i parametri che veranno utilizzati per tutte le nostre reti.\n",
        " Il\n",
        "``model_name`` input è il nome del model che scegliam di utilizzare\n",
        "::\n",
        "\n",
        "   [alexnet, vgg]\n",
        "\n",
        " ``num_classes`` rappresenta il numero di classe del dataset , ``batch_size`` rapresenta il  batch size utilizzato per il training , ``num_epochs`` numero di epoche per il training  che vogliamo utulizzare,\n",
        " ``feature_extract``  è un boleano che rapresenta si si vuole fare il fine tuning opure la feature extration . se ``feature_extract = False``,il modello è finetune quindi tutti i parametri del modello  sarano aggiornati. Se\n",
        "``feature_extract = True``,  solo gli ultimi parametri del layer vengono aggiornati, gli altri rimangono fissi.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiyOYrEE6vre"
      },
      "source": [
        "# Top level data directory. Here we assume the format of the directory conforms \n",
        "#   to the ImageFolder structure\n",
        "data_dir = \"/content/gdrive/MyDrive/DataSet\"\n",
        "\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"alexnet\"\n",
        "#model_name = \"vgg\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 8\n",
        "\n",
        "# Number of epochs to train for \n",
        "num_epochs = 5\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model, \n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwyEKvmreDzC"
      },
      "source": [
        "Inizializzazione librerie per la face detection "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtci4cYgkvbN",
        "outputId": "2b32403c-b5e1-4e70-edf1-423463a672a9"
      },
      "source": [
        "#@title \n",
        "!pip install -q face_recognition\n",
        "!pip install -q fer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.patches as patches\n",
        "%matplotlib inline\n",
        "import face_recognition\n",
        "import numpy as np\n",
        "from PIL import ImageDraw\n",
        "from PIL import Image\n",
        "import PIL.Image\n",
        "from io import BytesIO\n",
        "from fer import FER\n",
        "import cv2\n",
        "import operator\n",
        "import time\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "detector = FER()\n",
        "import os       ## For Reading the file name\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from array import array\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlhGAFgazWMN"
      },
      "source": [
        "# Face detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t5RpPuekNoh"
      },
      "source": [
        "db_path = '/content/morph'\n",
        "\n",
        "def read_img(path):\n",
        "   img = cv2.imread(path) ## reading image\n",
        "   (h,w) = img.shape[:2]  ## fetching height and width\n",
        "   width = 500            ## hard coding width\n",
        "   ratio = width / float(w) ## preparing a ration for height\n",
        "   height = int(h * ratio)  ## generating new height\n",
        "   return cv2.resize(img,(width,height)) ##return the reshaped image\n",
        "\n",
        "\n",
        "known_encodings = [] \n",
        "known_names = []\n",
        "directory = '/content/londondb_morph_combined_alpha0.5_passport-scale_15kb'\n",
        "trainM='/content/gdrive/MyDrive/DataSet/train/MorphedT'\n",
        "trainB='/content/gdrive/MyDrive/DataSet/train/bona_fedeT'\n",
        "testM='/content/gdrive/MyDrive/DataSet/val/MorphedV'\n",
        "testB='/content/gdrive/MyDrive/DataSet/val/bona_fedeV'\n",
        "prova='/content/gdrive/MyDrive/DataSet/trainMorphed'\n",
        "import skimage.io as io\n",
        "\n",
        "for file in os.listdir(trainM):\n",
        "  img = read_img(os.path.join(trainM, file))\n",
        "  face_locations = face_recognition.face_locations(img)\n",
        "  known_names.append(file.split('.')[0])\n",
        "  known_encodings.append(face_locations)\n",
        "  \n",
        "  for (top, right, bottom, left) in face_locations:\n",
        "        top, right, bottom, left = face_recognition.face_locations(img)[0]\n",
        "        cv2.rectangle(img, (right, bottom),(left, top), (0, 0, 255), 2)\n",
        "        \n",
        "        #Crop selected roi from raw image\n",
        "        #pil=Image.fromarray(images[0])\n",
        "        #y en haut=top,bottom ,left,x large a gauche=right\n",
        "        #cropped_image = img[170:439, 82:350]\n",
        "        cropped_image = img[top:bottom, left:right]\n",
        "        gray = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
        "        #cv2.imwrite(file, cropped_image)\n",
        "        cv2.imwrite(os.path.join(prova,file),cropped_image)\n",
        "        cv2.waitKey(0)\n",
        "       # risized=cv2.resize(gray, (227, 227), interpolation=cv2.INTER_LINEAR)\n",
        "        #cv2_imshow(risized)\n",
        "        #cv2_imshow(gray)\n",
        "        #cv2_imshow(img)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4KI22uU6vro"
      },
      "source": [
        "metodo training e validation \n",
        "----------------\n",
        "\n",
        "Prima di scrivere il codice per la regolazione dei due modelli, definiamone alcuni funzioni di supporto.\n",
        "\n",
        "Codice di training e validation  del modello\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWXBOLQZ6vr3"
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "#\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBF7Nfya6vr8"
      },
      "source": [
        "Set Model Parameters’ .requires_grad attribute\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMZsZRt86vsA"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuIKTQN16vsI"
      },
      "source": [
        "Inizializzione e rimodellamento delle reti\n",
        "-----------------------------------\n",
        "Qui è dove gestiamo il rimodellamento di ciascuna rete. noi cambiamo il livello fully connected che initialmente essendo preadestrato su imagenet era a 1000 classi di output in due classi per la classifiazine del nostro modello.Quando si estraggono le feature, vogliamo solo aggiornare i parametri dell'ultimo layer, o in altre parole, vogliamo solo aggiornare i parametri per i layer che stiamo rimodellando. Pertanto, non abbiamo bisogno di calcolare i gradienti dei parametri che non stiamo modificando, quindi per l'efficienza impostiamo l'attributo .requires_grad su False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549,
          "referenced_widgets": [
            "818f74c8bda94a40b42c04adbf5179c6",
            "9220cad100b2440596411c777a819415",
            "0667bd76dac64244966fb9e80158df8f",
            "c16109a7fa0e4f66956c5909fd8fb729",
            "0b3e9ed2c1b14316b2e4806043bb78a6",
            "3f974598d2cb4536bf6af718ac397e96",
            "475be7b5453f453dac60cb607314dbcd",
            "5f185e712dce4464af37870b77155baa",
            "41de7dcf7fd345e1b1aa35d020c69651",
            "aafdd2f18ab3451db7709445bc73372c",
            "7ab04b14dbba4e19957c05ce54ba0dea"
          ]
        },
        "id": "-UILGQZq6vsM",
        "outputId": "992e742f-d6d9-4454-eb1d-bf0a02e296c2"
      },
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "    if model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 227\n",
        "        layer = model_ft._modules.get('avgpool')\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG19_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg19_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    \n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "818f74c8bda94a40b42c04adbf5179c6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN6Z-xgf6vsO"
      },
      "source": [
        "Pre-prosessing e data aumentation\n",
        "---------\n",
        "in questa fase,vengono effetuati trasformazioni dei dati, i set di dati delle immagini e i dataloader. Notare che i modelli sono stati preaddestrati con alcuni valori di normalizzazione che abbiamo resprttato .\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9f6v2ig6vsQ",
        "outputId": "65af46c4-c8ad-490a-d5df-2d4e5f676317"
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "if model_name == \"alexnet\":\n",
        "    #test\n",
        "    data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        #Resize the image to 256×256 pixels.\n",
        "        transforms.Resize(256),\n",
        "        # Crop the image to 224×224 pixels about the center.\n",
        "        transforms.CenterCrop(227),\n",
        "        # Convert the image to PyTorch Tensor data type.\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.RandomAffine(degrees =0, translate = (0, +1)),\n",
        "        #Normalize the image by setting its mean and standard deviation to the specified values.\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "elif model_name == \"vgg\":\n",
        "    data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        #Resize the image to 256×256 pixels.\n",
        "        transforms.Resize(256),\n",
        "        # Crop the image to 224×224 pixels about the center.\n",
        "        transforms.CenterCrop(224),\n",
        "        # Convert the image to PyTorch Tensor data type.\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        #Normalize the image by setting its mean and standard deviation to the specified values.\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqaxt2246vsY"
      },
      "source": [
        "Creazione dell'Optimizer\n",
        "--------------------\n",
        "Ora che la struttura del modello è corretta, il passaggio finale per la messa a punto\n",
        "per l'estrazione delle funzionalità consiste nel creare un ottimizzatore che aggiorni solo il parametri desiderati.Quindi tutti i parametri che hanno .requeres_grad=True dovrebbe essere ottimizzato .Poi facciamo un elenco di tali parametri e gli inseriamo nel costruttore dell'algoritmo SGD\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jBAT-JD6vsb",
        "outputId": "65fdc288-651f-441a-de7d-84b2e57f45d4"
      },
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t classifier.6.weight\n",
            "\t classifier.6.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDYxmTgf6vsf"
      },
      "source": [
        "Running del modello di Training e Validation\n",
        "--------------------------------\n",
        "\n",
        "Infine, l'ultimo passaggio consiste nell'impostare la loss  per il modello,Poi eseguire il training e il validation  per il numero di epoche impostato.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PTakiTZ6vsg",
        "outputId": "6d434349-4be5-4321-fe33-64ac2e3f808e"
      },
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.2640 Acc: 0.9478\n",
            "val Loss: 0.1840 Acc: 0.9516\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 0.2213 Acc: 0.9501\n",
            "val Loss: 0.2106 Acc: 0.9537\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 0.2163 Acc: 0.9495\n",
            "val Loss: 2.3394 Acc: 0.1095\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 0.2259 Acc: 0.9456\n",
            "val Loss: 0.2415 Acc: 0.9158\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 0.2376 Acc: 0.9473\n",
            "val Loss: 0.9561 Acc: 0.5221\n",
            "\n",
            "Training complete in 7m 33s\n",
            "Best val Acc: 0.953684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4mQhNS-BU6f"
      },
      "source": [
        "model_path =\"./content\"\n",
        "torch.save(model_ft.state_dict(), \"alexnetModel.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05czGyVZ6vsh"
      },
      "source": [
        "Confronto con il modello addestrato da zero\n",
        "------------------------------------------\n",
        "\n",
        "Solo per motivo di test , proviamo a addestrare i modeli da zero e vediamo come avvengono i risultati.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "gsNLjaaE6vsl",
        "outputId": "6cffade6-3e77-4d93-f1cb-a3a520dc626b"
      },
      "source": [
        "# Initialize the non-pretrained version of the model used for this run\n",
        "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
        "scratch_model = scratch_model.to(device)\n",
        "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
        "scratch_criterion = nn.CrossEntropyLoss()\n",
        "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
        "\n",
        "# Plot the training curves of validation accuracy vs. number \n",
        "#  of training epochs for the transfer learning method and\n",
        "#  the model trained from scratch\n",
        "ohist = []\n",
        "shist = []\n",
        "\n",
        "ohist = [h.cpu().numpy() for h in hist]\n",
        "shist = [h.cpu().numpy() for h in scratch_hist]\n",
        "\n",
        "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
        "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
        "plt.ylim((0,1.))\n",
        "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.2839 Acc: 0.9562\n",
            "val Loss: 0.1957 Acc: 0.9516\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 0.1878 Acc: 0.9562\n",
            "val Loss: 0.2044 Acc: 0.9516\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 0.1878 Acc: 0.9562\n",
            "val Loss: 0.1991 Acc: 0.9516\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 0.1903 Acc: 0.9562\n",
            "val Loss: 0.2228 Acc: 0.9516\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 0.1876 Acc: 0.9562\n",
            "val Loss: 0.1971 Acc: 0.9516\n",
            "\n",
            "Training complete in 21m 42s\n",
            "Best val Acc: 0.951579\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDIdfv5ha7nr"
      },
      "source": [
        "torch.save(scratch_model.state_dict(), \"alexnetModelScratch.pth\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGEsMOuo-GJF"
      },
      "source": [
        "# Matrice di confusione"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqCndB7Ht7yV"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "# iterate over test data\n",
        "for inputs,labels in dataloaders_dict['val']:\n",
        "        output = model_ft(inputs) # Feed Network\n",
        "\n",
        "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
        "        y_pred.extend(output) # Save Prediction\n",
        "        \n",
        "        labels = labels.data.cpu().numpy()\n",
        "        y_true.extend(labels) # Save Truth\n",
        "\n",
        "# constant for classes\n",
        "classes = ('bona_fede','Morphed')\n",
        "\n",
        "# Build confusion matrix\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
        "                     columns = [i for i in classes])\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.savefig('output.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}