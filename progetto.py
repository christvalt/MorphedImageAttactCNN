# -*- coding: utf-8 -*-
"""progetto.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z5i17PHKHWDXS5YicDpGYszi2NWxFVc_

Progetto image morphing

**Import delle librerie**

Per prima cosa Ã¨ necessario eseguire l'import delle librerie utilizzate durante l'esecitazione.
"""

# Commented out IPython magic to ensure Python compatibility.
#@title 
!pip install -q face_recognition
!pip install -q fer

import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow
import matplotlib.patches as patches
# %matplotlib inline
import face_recognition
import numpy as np
from PIL import ImageDraw
import PIL.Image
from io import BytesIO
from fer import FER
import cv2
import operator
import time
from google.colab import files
from google.colab.patches import cv2_imshow
detector = FER()

"""Operazioni preliminari"""

!wget http://bias.csr.unibo.it/franco/AMSL_FaceMorphImageDataSet.zip
!wget http://bias.csr.unibo.it/VR/Esercitazioni/PythonUtilities.zip

!unzip /content/AMSL_FaceMorphImageDataSet.zip
!unzip /content//PythonUtilities.zip



db_path = '/content/londondb_genuine_neutral_passport-scale_15kb'
#@title 
import glob
#uploaded = files.upload() #https://unsplash.com/photos/1qfy-jDc_jo

files = glob.glob("/content/londondb_genuine_neutral_passport-scale_15kb/*.jpg")
img_a = cv2.imread(files[0])

for file in files[1:]:

    img_b = cv2.imread(file);

len(img_b)  
img_b

uploaded
all=[]
all.append(uploaded)
all

"""# Display image


"""

#@title Display image
image = face_recognition.load_image_file(list(uploaded.keys())[0])
boundary = 24
im = PIL.Image.open(list(uploaded.keys())[0])
height = (im.size[1] / im.size[0]) * 20
fig=plt.subplots(figsize=(20,height))
imshow(im)

"""# Use 'Face Recognition' library and draw a red box around the faces as well as predicted emotion"""

#@title 
start = time.time()
face_locations = face_recognition.face_locations(img_b)
fig,ax = plt.subplots(figsize=(20,height))
for face_location in face_locations:
    top, right, bottom, left = face_location 
    ax.imshow(im,aspect='auto')
    rect = patches.Rectangle((left,top), (right - left), (bottom-top),linewidth=3,edgecolor='r',facecolor='none')
    ax.add_patch(rect)
    face_image = img_b[top-boundary:bottom+boundary, left-boundary:right+boundary]
    attribute = detector.detect_emotions(face_image)
    if (not attribute) == False:
      emotion = max(attribute[0]['emotions'].items(),key=operator.itemgetter(1))[0]
      #print(emotion)
      plt.text(left, top, emotion, fontsize=8, bbox=dict(fill=True, edgecolor='blue', linewidth=1))
end = time.time()
print("Number of faces:",len(face_locations))
print("Time taken:",end-start," seconds")

"""# Use Haar Cascades and draw a red box around the faces"""

#@title 
start = time.time()
#Load the cascade
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

#Read the input image
img = cv2.imread(list(uploaded.keys())[0])

#Convert into GrayScale
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#Detect Faces
faces = face_cascade.detectMultiScale(gray, 1.3, 4)
#Draw rectangle around each faces
for (x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 3)
#Display the output
ims = cv2.resize(img,(im.size[0],im.size[1]))
cv2_imshow(ims)
cv2.waitKey()
end = time.time()
print("Time taken:",end-start," seconds")
print("Number of faces:",len(faces))